{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maniappan/linuxscripts/blob/master/anomaly_detection_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457601ba-5e44-4e32-9a4b-9c214e63edff",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "457601ba-5e44-4e32-9a4b-9c214e63edff",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "457601ba-5e44-4e32-9a4b-9c214e63edff",
        "outputId": "ef6dd7ae-fb47-4f5f-e93e-2941042f83b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# https://youtu.be/P9NdQG_vIvo\n",
        "\"\"\"\n",
        "Anomaly localization in images using the global average pooling layer.\n",
        "\n",
        "Binary classification - Good vs. bad images (Uninfected vs parasiized)\n",
        "\n",
        "This code uses the malarial data set but it can be easily applied to\n",
        "any application.\n",
        "\n",
        "Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html\n",
        "\"\"\"\n",
        "\n",
        "##########################################################\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import scipy  # Used to upsample our image\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "### Set GPUs ###\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "gpus\n",
        "print(gpus)\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "tf.debugging.set_log_device_placement(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NAjGHFO2BEvF"
      },
      "id": "NAjGHFO2BEvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffd21f9-e242-4074-a20e-5ddf57c40148",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "2ffd21f9-e242-4074-a20e-5ddf57c40148",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "2ffd21f9-e242-4074-a20e-5ddf57c40148",
        "outputId": "6d3f3f27-8c78-4e98-eea7-366361009baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got the datasets\n"
          ]
        }
      ],
      "source": [
        "# Read images and get them ready for training\n",
        "\n",
        "image_directory = 'cellimg/cell_images/'\n",
        "SIZE = 224\n",
        "dataset = []  # Many ways to handle data, you can use pandas. Here, we are using a list format.\n",
        "label = []  # Placeholders to define add labels. We will add 1 to all parasitized images and 0 to uninfected.\n",
        "\n",
        "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
        "for i, image_name in enumerate(\n",
        "        parasitized_images):  # Remember enumerate method adds a counter and returns the enumerate object\n",
        "\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(1)\n",
        "\n",
        "# Iterate through all images in Uninfected folder, resize to 224x224\n",
        "# Then save into the same numpy array 'dataset' but with label 0\n",
        "\n",
        "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
        "for i, image_name in enumerate(uninfected_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(0)\n",
        "print(\"Got the datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9270da70-5373-40a5-865e-e380a0568e31",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "9270da70-5373-40a5-865e-e380a0568e31",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "9270da70-5373-40a5-865e-e380a0568e31"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "label = np.array(label)\n",
        "\n",
        "# Split into train and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size=0.20, random_state=0)\n",
        "\n",
        "# Without scaling (normalize) the training may not converge.\n",
        "# so that all values are within the range of 0 and 1.\n",
        "\n",
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255.\n",
        "\n",
        "# Let us setup the model as multiclass with total classes as 2.\n",
        "# This way the model can be used for other multiclass examples.\n",
        "# Since we will be using categorical cross entropy loss, we need to convert our Y values to categorical.\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(\"Train data to Categorical\")\n",
        "y_train = to_categorical(y_train)\n",
        "print(\"Test data to categorical\")\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b01294-6abf-43ce-8f5e-861e1126e337",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "b4b01294-6abf-43ce-8f5e-861e1126e337",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "b4b01294-6abf-43ce-8f5e-861e1126e337"
      },
      "outputs": [],
      "source": [
        "# Define the model.\n",
        "# Here, we use pre-trained VGG16 layers and add GlobalAveragePooling and dense prediction layers.\n",
        "# You can define any model.\n",
        "# Also, here we set the first few convolutional blocks as non-trainable and only train the last block.\n",
        "# This is just to speed up the training. You can train all layers if you want.\n",
        "def get_model(input_shape=(224, 224, 3)):\n",
        "    vgg = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # for layer in vgg.layers[:-8]:  #Set block4 and block5 to be trainable.\n",
        "    for layer in vgg.layers[:-5]:  # Set block5 trainable, all others as non-trainable\n",
        "        print(layer.name)\n",
        "        layer.trainable = False  # All others as non-trainable.\n",
        "\n",
        "    x = vgg.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Use GlobalAveragePooling and NOT flatten.\n",
        "    x = Dense(2, activation=\"softmax\")(x)  # We are defining this as multiclass problem.\n",
        "    print (\"Model compile\")\n",
        "    model = Model(vgg.input, x)\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd88a6cb-bc1f-486c-b600-207df77f4208",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "dd88a6cb-bc1f-486c-b600-207df77f4208",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "dd88a6cb-bc1f-486c-b600-207df77f4208"
      },
      "outputs": [],
      "source": [
        "print (\"Start model build\")\n",
        "model = get_model(input_shape=(224, 224, 3))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac943ddd-9f9f-44ac-95e6-8be7ea16b891",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ac943ddd-9f9f-44ac-95e6-8be7ea16b891",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "ac943ddd-9f9f-44ac-95e6-8be7ea16b891"
      },
      "outputs": [],
      "source": [
        "print (\"Model fit\")\n",
        "history = model.fit(X_train, y_train, batch_size=16, epochs=1, verbose=1,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "# plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'y', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2a1c96-ee04-40b2-b3ec-60f9803a3cf4",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "7a2a1c96-ee04-40b2-b3ec-60f9803a3cf4",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "7a2a1c96-ee04-40b2-b3ec-60f9803a3cf4",
        "outputId": "80ea74af-9db8-4a82-c93f-9d81716f360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting testdata\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_280/1648972263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting testdata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check model accuracy on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ],
      "source": [
        "###############################################################\n",
        "print(\"Starting testdata\")\n",
        "# Check model accuracy on the test data\n",
        "_, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
        "\n",
        "# Test on single image.\n",
        "n = 10  # Select the index of image to be loaded for testing\n",
        "img = X_test[n]\n",
        "plt.imshow(img)\n",
        "input_img = np.expand_dims(img, axis=0)  # Expand dims so the input is (num images, x, y, c)\n",
        "print(\"The prediction for this image is: \", np.argmax(model.predict(input_img)))\n",
        "print(\"The actual label for this image is: \", np.argmax(y_test[n]))\n",
        "\n",
        "# Print confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "print (\"confusion matrix\")\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "sns.heatmap(cm, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca581339-8684-44f0-b8d2-c826a3e4510f",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "ca581339-8684-44f0-b8d2-c826a3e4510f",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "ca581339-8684-44f0-b8d2-c826a3e4510f",
        "outputId": "63d99280-9aa4-45db-866b-601f34d7025c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_280/3004119713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Identify all images classified as parasitized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparasited_image_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving all images as parasited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Save all images classified as parasited to a directory (optional, makes sense for large data sets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#############################################################\n",
        "# Save all images classified as parasited so we can fetch these images\n",
        "# later and plot heatmaps.\n",
        "########################################################\n",
        "# Identify all images classified as parasitized\n",
        "parasited_image_idx = np.where(y_pred == 1)[0]\n",
        "print(\"saving all images as parasited\")\n",
        "# Save all images classified as parasited to a directory (optional, makes sense for large data sets)\n",
        "# capture it in memory as an array\n",
        "predicted_as_para = []\n",
        "for i in parasited_image_idx:\n",
        "    par_img = X_test[i]\n",
        "    # plt.imsave(\"results_classified_as_para/para_\"+str(i)+\".png\", par_img)\n",
        "    predicted_as_para.append(par_img)\n",
        "\n",
        "predicted_as_para = np.array(predicted_as_para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d650a1c-d31b-499a-abb0-0b59526af7c3",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "7d650a1c-d31b-499a-abb0-0b59526af7c3",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "7d650a1c-d31b-499a-abb0-0b59526af7c3",
        "outputId": "4eb1205b-cfac-4960-9653-2b0d3da9cf42"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'skimage'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_280/1114973518.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRectangle\u001b[0m  \u001b[0;31m# To add a rectangle overlay to the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeak\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeak_local_max\u001b[0m  \u001b[0;31m# To detect hotspots in 2D images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prepare heatmap\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
          ]
        }
      ],
      "source": [
        "from matplotlib.patches import Rectangle  # To add a rectangle overlay to the image\n",
        "from skimage.feature.peak import peak_local_max  # To detect hotspots in 2D images.\n",
        "\n",
        "print(\"prepare heatmap\")\n",
        "def plot_heatmap(img):\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_class = np.argmax(pred)\n",
        "    # Get weights for all classes from the prediction layer\n",
        "    last_layer_weights = model.layers[-1].get_weights()[0]  # Prediction layer\n",
        "    # Get weights for the predicted class.\n",
        "    last_layer_weights_for_pred = last_layer_weights[:, pred_class]\n",
        "    # Get output from the last conv. layer\n",
        "    last_conv_model = Model(model.input, model.get_layer(\"block5_conv3\").output)\n",
        "    last_conv_output = last_conv_model.predict(img[np.newaxis, :, :, :])\n",
        "    last_conv_output = np.squeeze(last_conv_output)\n",
        "\n",
        "    # Upsample/resize the last conv. output to same size as original image\n",
        "    h = int(img.shape[0] / last_conv_output.shape[0])\n",
        "    w = int(img.shape[1] / last_conv_output.shape[1])\n",
        "    upsampled_last_conv_output = scipy.ndimage.zoom(last_conv_output, (h, w, 1), order=1)\n",
        "    print(\"form heat maps\")\n",
        "    heat_map = np.dot(upsampled_last_conv_output.reshape((img.shape[0] * img.shape[1], 512)),\n",
        "                      last_layer_weights_for_pred).reshape(img.shape[0], img.shape[1])\n",
        "\n",
        "    # Since we have a lot of dark pixels where the edges may be thought of as\n",
        "    # high anomaly, let us drop all heat map values in this region to 0.\n",
        "    # This is an optional step based on the image.\n",
        "    heat_map[img[:, :, 0] == 0] = 0  # All dark pixels outside the object set to 0\n",
        "\n",
        "        # Detect peaks (hot spots) in the heat map. We will set it to detect maximum 5 peaks.\n",
        "    # with rel threshold of 0.5 (compared to the max peak).\n",
        "    peak_coords = peak_local_max(heat_map, num_peaks=5, threshold_rel=0.5, min_distance=10)\n",
        "    print (\"Should show heatmap now\")\n",
        "    plt.imshow(img.astype('float32').reshape(img.shape[0], img.shape[1], 3))\n",
        "    plt.imshow(heat_map, cmap='jet', alpha=0.30)\n",
        "    for i in range(0, peak_coords.shape[0]):\n",
        "        print(i)\n",
        "        y = peak_coords[i, 0]\n",
        "        x = peak_coords[i, 1]\n",
        "        print(\"Show the patch now\")\n",
        "        plt.gca().add_patch(Rectangle((x - 25, y - 25), 50, 50, linewidth=1, edgecolor='r', facecolor='none'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d176aaa-b796-457a-9a0b-4d8fe30953ad",
      "metadata": {
        "gradient": {
          "editing": false,
          "id": "2d176aaa-b796-457a-9a0b-4d8fe30953ad",
          "kernelId": "12fa1c12-5ac9-423c-a7eb-1df6c8fdb679"
        },
        "id": "2d176aaa-b796-457a-9a0b-4d8fe30953ad",
        "outputId": "d4e1a77e-5d3b-487e-8ca7-f5cd86f1ee4b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'predicted_as_para' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_280/2715941358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_as_para\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mheat_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_as_para\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"show the predicted one\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predicted_as_para' is not defined"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "im = random.randint(0, predicted_as_para.shape[0] - 1)\n",
        "heat_map = plot_heatmap(predicted_as_para[im])\n",
        "print(\"show the predicted one\")\n",
        "img = predicted_as_para[im]\n",
        "plt.imshow(predicted_as_para[im])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "anomaly_detection_gpu.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}